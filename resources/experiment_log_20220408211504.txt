{"ENCODER_INPUT_WIDTH": 255, "ENCODER_INPUT_HEIGHT": 255, "LATENT_SPACE_SIZE": 8, "LEARNING_RATE": 1e-06, "EPOCHS": 10, "BATCH_SIZE": 32, "DATA_PATH": "/home/joseph/512/", "ARCHITECTURE": "Sequential(\n  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (2): LeakyReLU(negative_slope=0.01, inplace=True)\n  (3): AvgPool2d(kernel_size=3, stride=3, padding=0)\n  (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): LeakyReLU(negative_slope=0.01, inplace=True)\n  (7): AvgPool2d(kernel_size=3, stride=3, padding=0)\n  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (9): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (10): LeakyReLU(negative_slope=0.01, inplace=True)\n  (11): AvgPool2d(kernel_size=3, stride=3, padding=0)\n  (12): Flatten(start_dim=1, end_dim=-1)\n  (13): Linear(in_features=10368, out_features=1024, bias=True)\n  (14): Linear(in_features=1024, out_features=1024, bias=True)\n  (15): Linear(in_features=1024, out_features=8, bias=True)\n  (16): Tanh()\n)", "NOTES": "New dataset has made the model much more selective, but recall is a little lower.  Trying an extra dense layer.  Next, I think we should omit the tanh output to see if we get a distribution in an n-dimensional hyperspace instead of on the surface of a hypersphere.", "TRAINING_LOSSES": [9779.754776958376, 8335.017551044002, 7896.069427996874, 7653.392191082239, 7541.811741698533, 7455.088150821626, 7375.218165034428, 7316.804589468986, 7259.456379633397, 7197.190549919382]}