{"ENCODER_INPUT_WIDTH": 255, "ENCODER_INPUT_HEIGHT": 255, "LATENT_SPACE_SIZE": 8, "LEARNING_RATE": 1e-06, "EPOCHS": 300, "BATCH_SIZE": 32, "DATA_PATH": "E:\\Pictures", "ARCHITECTURE": "Sequential(\n  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (2): LeakyReLU(negative_slope=0.01, inplace=True)\n  (3): AvgPool2d(kernel_size=3, stride=3, padding=0)\n  (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): LeakyReLU(negative_slope=0.01, inplace=True)\n  (7): AvgPool2d(kernel_size=3, stride=3, padding=0)\n  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (9): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (10): LeakyReLU(negative_slope=0.01, inplace=True)\n  (11): AvgPool2d(kernel_size=3, stride=3, padding=0)\n  (12): Flatten(start_dim=1, end_dim=-1)\n  (13): Linear(in_features=10368, out_features=1024, bias=True)\n  (14): Linear(in_features=1024, out_features=8, bias=True)\n  (15): Tanh()\n)", "NOTES": "Same as before, but more epochs.  Prev note: The smaller model is either garbage or insufficiently trained.  Losses were near zero at the end, so \n\t\tI have to speculate that it's just not big enough to capture the kinds of data in which we're interested.  I'm\n\t\tgoing to remove the horizontal and vertical random flips because I think it might be hurting the approximate\n\t\thashing.  Also using -1, 1 for cosine distance again."}